#!/bin/bash

# RemoveAnything Three-Expert MoE LoRA Training Script (é€‚é…æœ¬åœ°ç¯å¢ƒ)
# åŸºäºç”¨æˆ·æä¾›çš„å®é™…é…ç½®å‚æ•°

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0
# æ˜¾å­˜ä¼˜åŒ–è®¾ç½®
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
export TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0"

# =============================================================================
# æ•°æ®è·¯å¾„é…ç½® (ä½¿ç”¨ç”¨æˆ·çš„å®é™…è·¯å¾„)
# =============================================================================
TRAIN_JSON_PATH="/aicamera-mlp/fq_proj/datasets/Eraser/open_ROREM_RORD_COCO_v2/gt_added_mapping_sample_1000.json"
# VAL_JSON_PATH="/aicamera-mlp/fq_proj/datasets/Eraser/open_ROREM_RORD_COCO_v2/gt_added_mapping_val.json"
VAL_JSON_PATH="/aicamera-mlp/fq_proj/datasets/Eraser/user_remove_test_v1"  # ä¹Ÿå¯ä»¥æ˜¯è·¯å¾„ï¼šinput,maské…çš„
TRAIN_METADATA_FILE="/aicamera-mlp/fq_proj/datasets/Eraser/open_ROREM_RORD_COCO_v2/train_metadata.json"

# =============================================================================
# è¾“å‡ºè·¯å¾„é…ç½®
# =============================================================================
OUTPUT_DIR="/aicamera-mlp/fq_proj/results/removal_flux_three_experts_fixed_grpo"

# =============================================================================
# æ¢å¤è®­ç»ƒé€‰é¡¹ (ä¸¤è€…é€‰ä¸€)
# =============================================================================

# =============================================================================
# æ¨¡å‹è·¯å¾„é…ç½® (ä½¿ç”¨ç”¨æˆ·çš„å®é™…è·¯å¾„)
# =============================================================================
FLUX_FILL_ID="/aicamera-mlp/fq_proj/weights/modelscope/FLUX.1-Fill-dev"
FLUX_REDUX_ID="/aicamera-mlp/fq_proj/weights/modelscope/FLUX.1-Redux-dev"

# =============================================================================
# åˆ†è¾¨ç‡å’Œæ¡¶é…ç½®
# =============================================================================
RESOLUTION='512,512'
ASPECT_RATIO_BUCKETS='512,512'
# aspect_ratio_buckets='1024,1024;768,1360;1360,768;880,1168;1168,880;1248,832;832,1248'

# =============================================================================
# MoE ä¸“å®¶é…ç½®
# =============================================================================
NUM_EXPERTS=3
EXPERT_NAMES="removal_expert,background_expert,completion_expert"
ROUTING_STRATEGY="soft"  # Options: soft, hard, topk
ROUTING_TEMPERATURE=1.0

# =============================================================================
# 8-bitä¼˜åŒ–å™¨é…ç½® (èŠ‚çœæ˜¾å­˜çº¦50%)
# =============================================================================
# è®¾ç½®ä¸ºtrueå¯ç”¨8-bitä¼˜åŒ–å™¨ï¼Œfalseä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨
USE_8BIT_OPTIMIZER=true
# USE_8BIT_OPTIMIZER=false  # å–æ¶ˆæ³¨é‡Šè¿™è¡Œæ¥ç¦ç”¨8-bitä¼˜åŒ–å™¨

if [ "$USE_8BIT_OPTIMIZER" = "true" ]; then
    OPTIMIZER_TYPE="adamw8bit"
    echo "âš™ï¸  å·²å¯ç”¨8-bitä¼˜åŒ–å™¨ï¼Œå¯èŠ‚çœæ˜¾å­˜çº¦50%"
    echo "ğŸš¨ è¯·ç¡®ä¿å·²å®‰è£…bitsandbytes: pip install bitsandbytes"
else
    OPTIMIZER_TYPE="adamw"
    echo "ğŸ“Š ä½¿ç”¨æ ‡å‡†AdamWä¼˜åŒ–å™¨"
fi

# =============================================================================
# ä¸¤é˜¶æ®µè®­ç»ƒé…ç½® (å¯é€‰)
# =============================================================================
# å–æ¶ˆæ³¨é‡Šä»¥ä¸‹è¡Œæ¥è¿›è¡Œä¸“å®¶ç‰¹åŒ–è®­ç»ƒ:
# TASK_HINT="removal_only"      # ä¸“æ³¨è®­ç»ƒç§»é™¤ä¸“å®¶
# TASK_HINT="background_only"   # ä¸“æ³¨è®­ç»ƒèƒŒæ™¯ä¸“å®¶  
# TASK_HINT="completion_only"   # ä¸“æ³¨è®­ç»ƒè¡¥å…¨ä¸“å®¶
# ç•™ç©ºè¡¨ç¤ºè”åˆè®­ç»ƒæ‰€æœ‰ä¸“å®¶
TASK_HINT=""

# =============================================================================
# æŸå¤±å‡½æ•°æ§åˆ¶ - æ¶ˆèå®éªŒé…ç½®
# =============================================================================
# åŸºç¡€è®¾ç½® - åªå¯ç”¨base lossï¼ˆé»˜è®¤å®‰å…¨æ¨¡å¼ï¼‰
ENABLE_BASE_LOSS=true
ENABLE_MASK_INFO=false
ENABLE_BOUNDARY_LOSS=false
ENABLE_CONSISTENCY_LOSS=false
ENABLE_DETAIL_LOSS=false

# æŸå¤±æƒé‡é…ç½®
BASE_LOSS_WEIGHT=1.0
BOUNDARY_LOSS_WEIGHT=0.3
CONSISTENCY_LOSS_WEIGHT=0.5
DETAIL_LOSS_WEIGHT=0.2

# =============================================================================
# è®­ç»ƒè¶…å‚æ•°é…ç½® (åŸºäºç”¨æˆ·åŸå§‹é…ç½®)
# =============================================================================
LEARNING_RATE=5e-5
TRAIN_BATCH_SIZE=1
NUM_TRAIN_EPOCHS=20
GRADIENT_ACCUMULATION_STEPS=8  # è¿›ä¸€æ­¥å¢åŠ æ¢¯åº¦ç´¯ç§¯ï¼Œå‡å°‘å³°å€¼æ˜¾å­˜
VALIDATION_STEPS=1000           # å‡å°‘éªŒè¯é¢‘ç‡ï¼ŒèŠ‚çœæ˜¾å­˜
CHECKPOINT_STEPS=1000          # å‡å°‘æ£€æŸ¥ç‚¹é¢‘ç‡

# =============================================================================
# LoRAé…ç½® (åŸºäºç”¨æˆ·åŸå§‹é…ç½®)
# =============================================================================
USE_LORA=true
LORA_R=64
LORA_ALPHA=64
LORA_DROPOUT=0.0
TARGET_MODULES="(.*x_embedder|.*(?<!single_)transformer_blocks\.[0-9]+\.norm1\.linear|.*(?<!single_)transformer_blocks\.[0-9]+\.attn\.to_k|.*(?<!single_)transformer_blocks\.[0-9]+\.attn\.to_q|.*(?<!single_)transformer_blocks\.[0-9]+\.attn\.to_v|.*(?<!single_)transformer_blocks\.[0-9]+\.attn\.to_out\.0|.*(?<!single_)transformer_blocks\.[0-9]+\.ff\.net\.2|.*single_transformer_blocks\.[0-9]+\.norm\.linear|.*single_transformer_blocks\.[0-9]+\.proj_mlp|.*single_transformer_blocks\.[0-9]+\.proj_out|.*single_transformer_blocks\.[0-9]+\.attn.to_k|.*single_transformer_blocks\.[0-9]+\.attn.to_q|.*single_transformer_blocks\.[0-9]+\.attn.to_v|.*single_transformer_blocks\.[0-9]+\.attn.to_out)"

# =============================================================================
# å…¶ä»–é…ç½®
# =============================================================================
SEED=233
MIXED_PRECISION="bf16"

# =============================================================================
# åˆ›å»ºè¾“å‡ºç›®å½•
# =============================================================================
mkdir -p $OUTPUT_DIR

echo "========================================================================"
echo "RemoveAnything Three-Expert MoE LoRA Training"
echo "========================================================================"
echo "é…ç½®ä¿¡æ¯:"
echo "- ä¸“å®¶æ•°é‡: $NUM_EXPERTS"
echo "- ä¸“å®¶åç§°: $EXPERT_NAMES"
echo "- è·¯ç”±ç­–ç•¥: $ROUTING_STRATEGY"
echo "- è·¯ç”±æ¸©åº¦: $ROUTING_TEMPERATURE"
echo "- ä»»åŠ¡æç¤º: ${TASK_HINT:-'æ—  (è”åˆè®­ç»ƒ)'}"
echo "- è¾“å‡ºç›®å½•: $OUTPUT_DIR"
echo "- å­¦ä¹ ç‡: $LEARNING_RATE"
echo "- ä¼˜åŒ–å™¨: ${OPTIMIZER_TYPE:-adamw}${USE_8BIT_OPTIMIZER:+ (8-bit)}"
echo "- æ‰¹é‡å¤§å°: $TRAIN_BATCH_SIZE"
echo "- LoRAç§©: $LORA_R"
echo "========================================================================"

# =============================================================================
# è¿è¡Œä¸‰ä¸“å®¶MoEè®­ç»ƒ
# =============================================================================
# accelerate launch src/train_bucket_moe.py \
#   --train_json_path "$TRAIN_JSON_PATH" \
#   --val_json_path "$VAL_JSON_PATH" \
#   --output_dir "$OUTPUT_DIR" \
#   --aspect_ratio_buckets "$ASPECT_RATIO_BUCKETS" \
#   --flux_fill_id "$FLUX_FILL_ID" \
#   --flux_redux_id "$FLUX_REDUX_ID" \
#   --learning_rate $LEARNING_RATE \
#   --train_batch_size $TRAIN_BATCH_SIZE \
#   --num_train_epochs $NUM_TRAIN_EPOCHS \
#   --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
#   --validation_steps $VALIDATION_STEPS \
#   --checkpointing_steps $CHECKPOINT_STEPS \
#   --use_lora $USE_LORA \
#   --lora_r $LORA_R \
#   --lora_alpha $LORA_ALPHA \
#   --lora_dropout $LORA_DROPOUT \
#   --target_modules "$TARGET_MODULES" \
#   --mixed_precision $MIXED_PRECISION \
#   --enable_xformers_memory_efficient_attention \
#   --gradient_checkpointing \
#   --seed $SEED \
#   --optimizer_type "$OPTIMIZER_TYPE" \
#   --use_8bit_optimizer $USE_8BIT_OPTIMIZER \
#   --num_experts $NUM_EXPERTS \
#   --expert_names removal_expert background_expert completion_expert \
#   --routing_strategy "$ROUTING_STRATEGY" \
#   --routing_temperature $ROUTING_TEMPERATURE \
#   --task_hint "$TASK_HINT" \
#   --enable_base_loss \
#   --base_loss_weight $BASE_LOSS_WEIGHT \
#   --boundary_loss_weight $BOUNDARY_LOSS_WEIGHT \
#   --consistency_loss_weight $CONSISTENCY_LOSS_WEIGHT \
#   --detail_loss_weight $DETAIL_LOSS_WEIGHT \
#   --report_to "tensorboard" \
#   --logging_dir "$OUTPUT_DIR/logs" \
# #   --enable_mask_info \
# #   --enable_boundary_loss \
# #   --enable_consistency_loss \
# #   --enable_detail_loss \

# accelerate launch src/train_bucket_moe_simple.py \
#   --train_json_path "$TRAIN_JSON_PATH" \
#   --val_json_path "$VAL_JSON_PATH" \
#   --output_dir "$OUTPUT_DIR" \
#   --aspect_ratio_buckets "$ASPECT_RATIO_BUCKETS" \
#   --flux_fill_id "$FLUX_FILL_ID" \
#   --flux_redux_id "$FLUX_REDUX_ID" \
#   --learning_rate $LEARNING_RATE \
#   --train_batch_size $TRAIN_BATCH_SIZE \
#   --num_train_epochs $NUM_TRAIN_EPOCHS \
#   --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
#   --validation_steps $VALIDATION_STEPS \
#   --checkpointing_steps $CHECKPOINT_STEPS \
#   --use_lora $USE_LORA \
#   --lora_r $LORA_R \
#   --lora_alpha $LORA_ALPHA \
#   --lora_dropout $LORA_DROPOUT \
#   --target_modules "$TARGET_MODULES" \
#   --mixed_precision $MIXED_PRECISION \
#   --enable_xformers_memory_efficient_attention \
#   --gradient_checkpointing \
#   --seed $SEED \
#   --optimizer_type "$OPTIMIZER_TYPE" \
#   --use_8bit_optimizer $USE_8BIT_OPTIMIZER \
#   --num_experts $NUM_EXPERTS \
#   --expert_names removal_expert background_expert completion_expert \
#   --routing_strategy "$ROUTING_STRATEGY" \
#   --routing_temperature $ROUTING_TEMPERATURE \
#   --report_to "tensorboard" \
#   --logging_dir "$OUTPUT_DIR/logs" \


# accelerate launch src/train_bucket_moe_fixed_grpo.py \
#   --train_json_path "$TRAIN_JSON_PATH" \
#   --val_json_path "$VAL_JSON_PATH" \
#   --output_dir "$OUTPUT_DIR" \
#   --aspect_ratio_buckets "$ASPECT_RATIO_BUCKETS" \
#   --flux_fill_id "$FLUX_FILL_ID" \
#   --flux_redux_id "$FLUX_REDUX_ID" \
#   --learning_rate $LEARNING_RATE \
#   --train_batch_size $TRAIN_BATCH_SIZE \
#   --num_train_epochs $NUM_TRAIN_EPOCHS \
#   --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
#   --validation_steps $VALIDATION_STEPS \
#   --checkpointing_steps $CHECKPOINT_STEPS \
#   --use_lora $USE_LORA \
#   --lora_r $LORA_R \
#   --lora_alpha $LORA_ALPHA \
#   --lora_dropout $LORA_DROPOUT \
#   --target_modules "$TARGET_MODULES" \
#   --mixed_precision $MIXED_PRECISION \
#   --enable_xformers_memory_efficient_attention \
#   --gradient_checkpointing \
#   --seed $SEED \
#   --optimizer_type "$OPTIMIZER_TYPE" \
#   --use_8bit_optimizer $USE_8BIT_OPTIMIZER \
#   --num_experts $NUM_EXPERTS \
#   --expert_names removal_expert background_expert completion_expert \
#   --routing_strategy "$ROUTING_STRATEGY" \
#   --routing_temperature $ROUTING_TEMPERATURE \
#   --report_to "tensorboard" \
#   --logging_dir "$OUTPUT_DIR/logs" \
#   --fixed_expert_weights 0.6 0.3 0.1 \
#   --enable_grpo_loss \
#   --use_grpo \
#   --base_loss_weight 1.0 \
#   --policy_loss_weight 0.1 \
#   --clip_range 0.2

# echo "========================================================================"
# echo "è®­ç»ƒå®Œæˆï¼"
# echo "æ£€æŸ¥ç‚¹ä¿å­˜åœ¨: $OUTPUT_DIR"
# echo "æ—¥å¿—ä¿å­˜åœ¨: $OUTPUT_DIR/logs"
# echo "========================================================================"


# =============================================================================
# GRPO åŠ¨æ€è·¯ç”±è®­ç»ƒ (æ¨èç”¨äºçœŸæ­£çš„å¼ºåŒ–å­¦ä¹ )
# =============================================================================

echo "========================================================================"
echo "GRPOåŠ¨æ€è·¯ç”±è®­ç»ƒå®Œæˆï¼"
echo "æ£€æŸ¥ç‚¹ä¿å­˜åœ¨: ${OUTPUT_DIR}_dynamic"
echo "æ—¥å¿—ä¿å­˜åœ¨: ${OUTPUT_DIR}_dynamic/logs"
echo "========================================================================"

# # ç®€åŒ–GRPOè®­ç»ƒï¼ˆåŸºäºåŸå§‹MoEè„šæœ¬çš„æœ€å°åŒ–é›†æˆï¼‰
# echo "å¼€å§‹ç®€åŒ–GRPOè®­ç»ƒï¼ˆåŸºäºåŸå§‹MoEè„šæœ¬ï¼‰..."
# echo "è¾“å‡ºç›®å½•: ${OUTPUT_DIR}_simple_grpo"

# accelerate launch src/train_bucket_moe_simple_grpo.py \
#   --train_json_path $TRAIN_JSON_PATH \
#   --val_json_path "$VAL_JSON_PATH" \
#   --output_dir "${OUTPUT_DIR}_simple_grpo" \
#   --aspect_ratio_buckets "$ASPECT_RATIO_BUCKETS" \
#   --flux_fill_id $FLUX_FILL_ID \
#   --flux_redux_id $FLUX_REDUX_ID \
#   --learning_rate $LEARNING_RATE \
#   --train_batch_size $TRAIN_BATCH_SIZE \
#   --num_train_epochs $NUM_TRAIN_EPOCHS \
#   --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
#   --validation_steps $VALIDATION_STEPS \
#   --checkpointing_steps $CHECKPOINT_STEPS \
#   --use_lora \
#   --lora_r $LORA_R \
#   --lora_alpha $LORA_ALPHA \
#   --lora_dropout $LORA_DROPOUT \
#   --target_modules "$TARGET_MODULES" \
#   --mixed_precision $MIXED_PRECISION \
#   --enable_xformers_memory_efficient_attention \
#   --gradient_checkpointing \
#   --seed $SEED \
#   --optimizer_type "$OPTIMIZER_TYPE" \
#   --use_8bit_optimizer \
#   --num_experts $NUM_EXPERTS \
#   --expert_names removal_expert,background_expert,completion_expert \
#   --routing_strategy "soft" \
#   --routing_temperature $ROUTING_TEMPERATURE \
#   --report_to "tensorboard" \
#   --logging_dir "${OUTPUT_DIR}_simple_grpo/logs" \
#   --enable_grpo_loss \
#   --use_grpo \
#   --base_loss_weight 1.0 \
#   --policy_loss_weight 0.1 \
#   --diversity_loss_weight 0.05 \
#   --clip_range 0.2 \
#   --removal_reward_weight 1.0 \
#   --background_reward_weight 0.5 \
#   --smoothness_reward_weight 0.3

# echo "========================================================================"
# echo "ç®€åŒ–GRPOè®­ç»ƒå®Œæˆï¼"
# echo "æ£€æŸ¥ç‚¹ä¿å­˜åœ¨: ${OUTPUT_DIR}_simple_grpo"
# echo "æ—¥å¿—ä¿å­˜åœ¨: ${OUTPUT_DIR}_simple_grpo/logs"
# echo "========================================================================"

# =============================================================================
# å›ºå®šæƒé‡GRPOè®­ç»ƒæµ‹è¯• (åŸºäºtrain_multi_lora_fixed_grpo.py)
# =============================================================================
echo "å¼€å§‹å›ºå®šæƒé‡GRPOè®­ç»ƒæµ‹è¯•..."
echo "è¾“å‡ºç›®å½•: ${OUTPUT_DIR}_fixed_grpo_test"

accelerate launch src/train_multi_lora_fixed_grpo.py \
  --train_json_path $TRAIN_JSON_PATH \
  --val_json_path "$VAL_JSON_PATH" \
  --output_dir "${OUTPUT_DIR}_fixed_grpo_test" \
  --aspect_ratio_buckets "$ASPECT_RATIO_BUCKETS" \
  --flux_fill_id $FLUX_FILL_ID \
  --flux_redux_id $FLUX_REDUX_ID \
  --learning_rate $LEARNING_RATE \
  --train_batch_size $TRAIN_BATCH_SIZE \
  --num_train_epochs $NUM_TRAIN_EPOCHS \
  --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
  --validation_steps $VALIDATION_STEPS \
  --checkpointing_steps $CHECKPOINT_STEPS \
  --use_8bit_optimizer \
  --use_lora True \
  --lora_r $LORA_R \
  --lora_alpha $LORA_ALPHA \
  --lora_dropout $LORA_DROPOUT \
  --target_modules "$TARGET_MODULES" \
  --mixed_precision $MIXED_PRECISION \
  --enable_xformers_memory_efficient_attention \
  --gradient_checkpointing \
  --seed $SEED \
  --optimizer_type "$OPTIMIZER_TYPE" \
  --lora_adapters "adapter1,adapter2,adapter3" \
  --adapter_weights "0.6,0.3,0.1" \
  --report_to "tensorboard" \
  --logging_dir "${OUTPUT_DIR}_fixed_grpo_test/logs" \
  --enable_grpo_loss \
  --use_grpo \
  --base_loss_weight 1.0 \
  --policy_loss_weight 0.1 \
  --diversity_loss_weight 0.05 \
  --clip_range 0.2 \
  --removal_reward_weight 1.0 \
  --background_reward_weight 0.5 \
  --smoothness_reward_weight 0.3

echo "========================================================================"
echo "å›ºå®šæƒé‡GRPOè®­ç»ƒæµ‹è¯•å®Œæˆï¼"
echo "æ£€æŸ¥ç‚¹ä¿å­˜åœ¨: ${OUTPUT_DIR}_fixed_grpo_test"
echo "æ—¥å¿—ä¿å­˜åœ¨: ${OUTPUT_DIR}_fixed_grpo_test/logs"
echo "========================================================================"
