# RemoveAnything Intelligent MoE LoRA Training

This document describes the **intelligent three-expert Mixture of Experts (MoE) LoRA architecture** for the RemoveAnything project, featuring **adaptive routing strategies** based on training tasks.

## ğŸ§  Intelligent Routing Strategy

### ğŸ¯ Core Innovation: Task-Aware MoE Activation

Our MoE system now intelligently switches between two modes based on the training configuration:

```python
# Smart routing decision
use_moe_routing = (
    args.use_lora and moe_manager is not None and 
    (args.enable_boundary_loss or args.enable_consistency_loss or args.enable_detail_loss)
)
```

### ğŸ“Š Two Operating Modes

#### ğŸ”„ **Mode 1: MoE Routing Mode** (Expert Specialization)
**Trigger**: When specialized losses are enabled
```bash
# Enable expert differentiation
--enable_boundary_loss      # Edge refinement signals
--enable_consistency_loss   # Background preservation signals  
--enable_detail_loss       # Detail preservation signals
```

**Behavior**:
- âœ… Dynamic routing weights based on mask analysis
- âœ… Expert specialization through distinct loss signals
- âœ… Routing statistics monitoring
- âœ… Task-aware weight distribution

#### âš–ï¸ **Mode 2: Uniform Weight Mode** (Traditional LoRA Effect)
**Trigger**: When only base MSE loss is enabled
```bash
# Basic training mode
--enable_base_loss          # Only MSE reconstruction loss
# (no additional specialized losses)
```

**Behavior**:
- âœ… Fixed uniform weights [1/3, 1/3, 1/3]
- âœ… Equivalent to traditional LoRA ensemble
- âœ… Computational efficiency
- âœ… No routing overhead

## ğŸ—ï¸ Expert Architecture

### Three Specialized Experts

1. **ğŸ¯ Removal Expert** (`removal_expert`)
   - **Specialization**: Complex object removal with occlusion handling
   - **Activated by**: High edge complexity, medium-large masks
   - **Optimized for**: Clean removal with seamless integration

2. **ğŸŒ„ Background Expert** (`background_expert`)
   - **Specialization**: Natural background generation and completion
   - **Activated by**: Large area masks, low detail complexity
   - **Optimized for**: Contextual background reconstruction

3. **ğŸ” Completion Expert** (`completion_expert`)
   - **Specialization**: Fine detail reconstruction and partial completion
   - **Activated by**: Small masks, high detail preservation needs
   - **Optimized for**: Texture and detail preservation

## ğŸš€ Quick Start

```bash
# Basic training with uniform weights (current config)
bash train_moe.sh
```

## ğŸ“Š Advanced Loss System

### ğŸ”„ Adaptive Loss Computation

Our system dynamically computes losses in different spaces for optimal performance:

```python
# Intelligent space selection
- base_loss: Computed in latent space (efficient)
- boundary_loss: Computed in pixel space (mask-aligned)
- consistency_loss: Computed in pixel space (mask-aligned) 
- detail_loss: Computed in pixel space (texture-aligned)
```

### ğŸ¯ Loss Components & Expert Signals

#### 1. **Base MSE Loss** (weight: 1.0)
- **Space**: Latent space
- **Purpose**: Overall reconstruction quality
- **Expert Signal**: Uniform (no differentiation)

#### 2. **Boundary Smoothness Loss** (weight: 0.3)
- **Space**: Pixel space (post-VAE decode)
- **Purpose**: Natural edge transitions
- **Expert Signal**: ğŸ¯ **Removal Expert** specialization

#### 3. **External Consistency Loss** (weight: 0.5)
- **Space**: Pixel space (post-VAE decode)
- **Purpose**: Background preservation
- **Expert Signal**: ğŸŒ„ **Background Expert** specialization

#### 4. **High-Frequency Detail Loss** (weight: 0.2)
- **Space**: Pixel space (post-VAE decode)
- **Purpose**: Texture and detail preservation
- **Expert Signal**: ğŸ” **Completion Expert** specialization

### âš™ï¸ Loss Configuration Strategies

#### Strategy A: Expert Differentiation Training
```bash
# Enable all losses for maximum expert specialization
--enable_base_loss \
--enable_boundary_loss --boundary_loss_weight=0.3 \
--enable_consistency_loss --consistency_loss_weight=0.5 \
--enable_detail_loss --detail_loss_weight=0.2 \
--enable_mask_info
```

#### Strategy B: Baseline Training  
```bash
# Only base loss for uniform weight training
--enable_base_loss --base_loss_weight=1.0
# (other losses disabled)
```

#### Strategy C: Custom Specialization
```bash
# Focus on specific aspects
--enable_base_loss \
--enable_boundary_loss --boundary_loss_weight=0.8  # High edge focus
--enable_mask_info
```

## ğŸ¯ Intelligent Mask-Aware Routing

### ğŸ§  Router Intelligence System

The router analyzes both **visual features** and **task requirements** to make routing decisions:

#### Input Features
```python
# Multi-modal routing input
hidden_states = cat([x_t, condition_latents], dim=2)  # Visual context
mask_info = {                                         # Task context
    'mask_latent': mask_in_latent_space,
    'area_ratio': mask_coverage_percentage,
    'edge_complexity': sobel_edge_analysis,
    'boundary_smoothness': edge_regularity_score
}
```

#### Routing Network Architecture
```python
class ThreeExpertRemovalRouter(nn.Module):
    def __init__(self, input_dim=128, hidden_dim=64):
        self.router = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 3)  # 3 experts
        )
```

### ğŸ“Š Routing Decision Logic

#### ğŸ”„ **Active Routing Mode**
```python
# When specialized losses provide clear signals
if boundary_loss or consistency_loss or detail_loss:
    weights = router.forward(hidden_states, mask_info)
    # Result: [0.2, 0.6, 0.2] - Background expert dominates
```

#### âš–ï¸ **Uniform Mode**
```python
# When only base loss is used
if only_base_loss:
    weights = [0.333, 0.333, 0.333]  # Equal contribution
    # Result: Traditional LoRA ensemble behavior
```

### ğŸ›ï¸ Routing Configuration

```bash
# Router hyperparameters
--routing_temperature=1.0    # Temperature for softmax (0.5-2.0)
--task_hint="removal_focus"  # Optional task guidance
```

### ğŸ“ˆ Routing Statistics Monitoring

```python
# Training logs show:
"expert_0_usage": 45,    # Removal expert usage count
"expert_1_usage": 78,    # Background expert usage count  
"expert_2_usage": 23,    # Completion expert usage count
"current_weights": [0.25, 0.55, 0.20]  # Current batch weights
```

## âš™ï¸ Key Parameters

```bash
# Essential MoE settings
--use_lora                 # Enable LoRA training
--num_experts=3           # Three expert architecture
--lora_r=256 --lora_alpha=256 --lora_dropout=0.0
--routing_temperature=1.0  # Routing decision sharpness

# Loss configuration (choose based on strategy)
--enable_base_loss --base_loss_weight=1.0         # Always enabled
--enable_boundary_loss --boundary_loss_weight=0.3  # For routing mode
--enable_consistency_loss --consistency_loss_weight=0.5 # For routing mode
--enable_detail_loss --detail_loss_weight=0.2      # For routing mode
--enable_mask_info                                 # Required for routing
```


## ğŸ“‹ Data Format

### ğŸ¯ Required Batch Structure

Your dataset must provide the following format:

```python
batch = {
    "ref": reference_image,      # [B, C, H, W] Reference image for Redux encoding
    "src": source_image,         # [B, C, H, W] Source image with object to remove  
    "mask": mask,               # [B, 1, H, W] Binary mask of region to remove
    "result": target_image      # [B, C, H, W] Target result image
}
```

### ğŸ“ Data Specifications

- **Image Resolution**: 512Ã—1024, 1024Ã—512, or 512Ã—512 (following Flux h,2*w convention)
- **Channels**: RGB images (3 channels)
- **Mask Format**: Single channel binary mask (0=keep, 1=remove)
- **Data Type**: float32 or float16
- **Normalization**: [0, 1] range (handled by VAE)

### ğŸ”„ Data Processing Pipeline

```python
# Training pipeline processing
1. ref â†’ flux_redux_pipe.image_encoder â†’ condition_latents
2. src â†’ vae.encode â†’ source_latents  
3. result â†’ vae.encode â†’ target_latents
4. mask â†’ vae.encode â†’ mask_latent (for loss computation)
```

## ğŸ”§ Advanced Usage

### ğŸ›ï¸ Loss Weight Tuning

æ ¹æ®æ•°æ®é›†ç‰¹ç‚¹è°ƒæ•´æŸå¤±æƒé‡ï¼š

```bash
# ç»†èŠ‚ä¸°å¯Œçš„æ•°æ®é›†
--boundary_loss_weight=0.8 --detail_loss_weight=0.5

# å¤æ‚èƒŒæ™¯çš„æ•°æ®é›†  
--consistency_loss_weight=0.8 --boundary_loss_weight=0.3

# ä¸è§„åˆ™å½¢çŠ¶çš„æ•°æ®é›†
--boundary_loss_weight=0.6 --consistency_loss_weight=0.5
```

### ğŸ“Š è®­ç»ƒç›‘æ§

**è·¯ç”±ç»Ÿè®¡** (ä»…åœ¨è·¯ç”±æ¨¡å¼ä¸‹):
- `expert_0/1/2_usage`: ä¸“å®¶ä½¿ç”¨æ¬¡æ•°
- `current_weights`: å½“å‰è·¯ç”±æƒé‡ (åº”è¯¥ä¼šåˆ†åŒ–)

**æŸå¤±ç»„ä»¶**:
- `loss/base`: åŸºç¡€é‡å»ºæŸå¤±
- `loss/boundary`: è¾¹ç•Œå¹³æ»‘æŸå¤±  
- `loss/consistency`: èƒŒæ™¯ä¿æŒæŸå¤±
- `loss/detail`: ç»†èŠ‚ä¿æŒæŸå¤±

### ğŸ”„ è®­ç»ƒæ¨¡å¼åˆ‡æ¢

å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ‡æ¢æ¨¡å¼ï¼š

```bash
# é˜¶æ®µ1: å‡åŒ€æƒé‡è®­ç»ƒ (0-1000æ­¥)
--enable_base_loss

# é˜¶æ®µ2: è·¯ç”±æ¨¡å¼è®­ç»ƒ (1000+æ­¥)
--enable_base_loss --enable_boundary_loss --enable_consistency_loss --enable_detail_loss --enable_mask_info
```

## ğŸ® Training Strategies

### ğŸ¯ Strategy 1: Progressive Specialization (Recommended)

#### Phase 1: Foundation Training (500-1000 steps)
```bash
# Build solid foundation with uniform weights
--enable_base_loss --base_loss_weight=1.0
# MoE operates in uniform weight mode [0.33, 0.33, 0.33]
```

#### Phase 2: Expert Differentiation (1000+ steps)
```bash
# Enable specialized losses for expert differentiation
--enable_base_loss --base_loss_weight=1.0 \
--enable_boundary_loss --boundary_loss_weight=0.3 \
--enable_consistency_loss --consistency_loss_weight=0.5 \
--enable_detail_loss --detail_loss_weight=0.2 \
--enable_mask_info
# MoE switches to active routing mode
```

### ğŸš€ Strategy 2: Direct Specialization

```bash
# Start with full specialization immediately
--enable_base_loss --base_loss_weight=1.0 \
--enable_boundary_loss --boundary_loss_weight=0.3 \
--enable_consistency_loss --consistency_loss_weight=0.5 \
--enable_detail_loss --detail_loss_weight=0.2 \
--enable_mask_info
```

### âš–ï¸ Strategy 3: Conservative Baseline

```bash
# Traditional LoRA-like training with uniform weights
--enable_base_loss --base_loss_weight=1.0
# Keep other losses disabled throughout training
```

## ğŸ“ˆ Monitoring and Debugging

### Key Metrics to Monitor

1. **Total Loss**: Overall training progress
2. **Component Losses**: Individual loss contributions
3. **Expert Usage**: Routing distribution across experts
4. **Boundary Quality**: Edge transition smoothness
5. **Consistency**: Background preservation quality

### Logging Output

```
INFO - Epoch 1/100, Step 100/5000
INFO - Total Loss: 0.1234, Base: 0.0800, Boundary: 0.0200, Consistency: 0.0150, Detail: 0.0084
INFO - Expert Usage: removal=0.45, background=0.35, completion=0.20
INFO - Routing Temperature: 1.0, Strategy: soft
```

## ğŸ” Troubleshooting

### Common Issues

1. **Expert Imbalance**: One expert dominates routing
   - **Solution**: Adjust routing temperature or use two-stage training

2. **Poor Boundary Quality**: Visible seams at mask edges
   - **Solution**: Increase boundary_weight in loss function

3. **Background Artifacts**: Unwanted changes outside mask
   - **Solution**: Increase consistency_weight in loss function

4. **Loss of Details**: Blurry or smoothed results
   - **Solution**: Increase detail_weight in loss function

### Debug Commands

```bash
# Check routing distribution
grep "Expert Usage" logs/training.log

# Monitor loss components  
grep "Component Losses" logs/training.log

# Validate model weights
python -c "
import torch
model = torch.load('outputs/checkpoint-1000/model.safetensors')
print([k for k in model.keys() if 'lora' in k])
"
```

## ğŸ“š Technical Details

### Architecture Changes

1. **Replaced** `FeatureExtractor` â†’ `MaskAwareFeatureExtractor`
2. **Replaced** `LoRARouter` â†’ `ThreeExpertRemovalRouter`  
3. **Added** `compute_removal_task_loss()` specialized loss function
4. **Added** `extract_mask_info_from_batch()` mask analysis
5. **Enhanced** MoE routing with mask awareness


## ğŸ¯ Expected Results

With proper training, you should see:

- **Specialized Expert Behavior**: Each expert handling its designated task type
- **Improved Boundary Quality**: Smoother transitions at removal edges  
- **Better Background Preservation**: Minimal artifacts outside mask regions
- **Enhanced Detail Retention**: Preserved texture and fine details
- **Intelligent Routing**: Appropriate expert selection based on mask characteristics


## ğŸ”§ Troubleshooting

### ğŸ› å¸¸è§é—®é¢˜

**è·¯ç”±æƒé‡ä¸å˜åŒ–**: ä»…å¯ç”¨äº†base_lossï¼Œéœ€è¦å¯ç”¨å…¶ä»–æŸå¤±æ¥æ¿€æ´»è·¯ç”±
**å†…å­˜ä¸è¶³**: é™ä½`--batch_size=1`ï¼Œå¯ç”¨`--gradient_checkpointing`
**æŸå¤±ç»„ä»¶ä¸æ˜¾ç¤º**: æ£€æŸ¥æ˜¯å¦å¯ç”¨`--enable_mask_info`

### ğŸ“ˆ æ€§èƒ½æç¤º

- **Batch Size**: ä»2å¼€å§‹ï¼Œæ ¹æ®æ˜¾å­˜è°ƒæ•´
- **Learning Rate**: ä¿å®ˆçš„1e-4ä¿è¯MoEç¨³å®šæ€§
- **Checkpointing**: æ¯500æ­¥ä¿å­˜æ£€æŸ¥ç‚¹

### ğŸ” è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
--logging_dir="./logs" --report_to="tensorboard"

# æŸ¥çœ‹ tensorboard æ—¥å¿—
tensorboard --logdir=./logs
```

## ğŸ“š èµ„æºæŒ‡å—

- **è®­ç»ƒè„šæœ¬**: `train_moe.sh` (è·¯ç”±æ¨¡å¼)
- **æ—¥å¿—æŸ¥çœ‹**: `tensorboard --logdir=./logs`
- **æ¨¡å‹ä¿å­˜**: æ¯ä¸ªä¸“å®¶é€‚é…å™¨åˆ†åˆ«ä¿å­˜

---

**ç¥–ä¼ é‡ã’ğŸš€ ç¥¥ç¦„ä¸æ¨¡å‹MoEçš„è®­ç»ƒï¼**
